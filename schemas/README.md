# JSON Schemas Documentation

This directory contains JSON Schema files that define and validate the structure of various data formats used throughout the test case management system.

## Overview

The schemas ensure data consistency and provide validation for:
- Test case definitions in YAML format
- Test execution logs
- Verification results
- Verification output files

## Schema Files

### 1. `test-case.schema.json`

**Purpose**: Validates test case YAML files against the GSMA test case structure.

**Description**: This schema defines the complete structure for test case definitions, including requirements, test sequences, steps, initial conditions, prerequisites, verification expressions, and environment variable hydration.

**Key Components**:
- **Root Properties**: `requirement`, `item`, `tc`, `id`, `description`
- **Prerequisites**: Manual or automatic prerequisites that must be satisfied before test execution
- **Hydration Variables**: Environment variables requiring configuration
- **Initial Conditions**: General and device-specific initial conditions (supports BDD patterns)
- **Test Sequences**: Ordered sequences containing test steps
- **Test Steps**: Individual test actions with commands, expected results, and verification rules
- **Verification**: Simple or conditional verification expressions for result/output validation

**Example Usage**:

```yaml
requirement: "XXX100"
item: 1
tc: 1
id: "TC_001"
description: "Example test case"

prerequisites:
  - type: automatic
    description: "SSH connection available"
    verification_command: "ssh -q user@host exit"

hydration_vars:
  TARGET_HOST:
    name: "TARGET_HOST"
    description: "Target device hostname"
    default_value: "192.168.1.1"
    required: true

general_initial_conditions:
  system:
    - "create directory \"/tmp/test\""
    - "set environment variable \"DEBUG\" to \"1\""

initial_conditions:
  device:
    - "ping device \"${TARGET_HOST}\" with 3 retries"

test_sequences:
  - id: 1
    name: "Basic Test"
    description: "Basic functionality test"
    initial_conditions:
      device:
        - "wait until port 80 on \"${TARGET_HOST}\" is open with timeout 30 seconds"
    steps:
      - step: 1
        description: "Execute command"
        command: "echo \"Hello\""
        expected:
          success: true
          result: 0
          output: "Hello"
        verification:
          result: "[[ $EXIT_CODE -eq 0 ]]"
          output: "grep -q 'Hello' <<< \"$COMMAND_OUTPUT\""
```

**Validation Tools**:
- `validate-yaml` binary: `validate-yaml testcase.yml --schema schemas/test-case.schema.json`
- Rust API: `SchemaValidator::new()?.validate_chunk(yaml_content)?`

**Related Documentation**:
- `docs/BDD_INITIAL_CONDITIONS.md` - BDD pattern reference for initial conditions
- `docs/CONDITIONAL_VERIFICATION.md` - Conditional verification expressions
- `docs/ENVIRONMENT_VARIABLE_HYDRATION.md` - Environment variable configuration
- `docs/PREREQUISITES.md` - Prerequisites system documentation
- `docs/VARIABLES_CAPTURE_COMMAND.md` - Variable capture functionality

---

### 2. `execution-log.schema.json`

**Purpose**: Validates test execution log entry format.

**Description**: This schema defines the structure for individual test execution log entries that record the execution of test steps. Each entry captures the command executed, its exit code, output, and timestamp.

**Key Components**:
- `test_sequence`: Sequence number of the test (integer)
- `step`: Step number within the sequence (integer)
- `command`: The command that was executed (string)
- `exit_code`: Exit code from command execution (integer)
- `output`: Output produced by the command (string)
- `timestamp`: ISO 8601 timestamp of execution (string, optional)

**Example**:

```json
[
  {
    "test_sequence": 1,
    "step": 1,
    "command": "echo \"Hello World\"",
    "exit_code": 0,
    "output": "Hello World",
    "timestamp": "2024-01-15T10:30:45.123456Z"
  },
  {
    "test_sequence": 1,
    "step": 2,
    "command": "false",
    "exit_code": 1,
    "output": "",
    "timestamp": "2024-01-15T10:30:45.234567Z"
  }
]
```

**Usage Context**:
- Generated by: `TestExecutor` during test case execution
- Used by: `TestVerifier` to compare actual vs. expected results
- Location: Execution log files are typically saved as `<test-case-id>_execution_log.json`

**Validation Tool**:
- `validate-json` binary: `validate-json execution_log.json schemas/execution-log.schema.json`

**Related Code**:
- `src/executor.rs` - Generates execution logs
- `src/verification.rs` - Parses and validates execution logs
- `models::TestStepExecutionEntry` - Rust structure representing log entries

---

### 3. `verification-result.schema.json`

**Purpose**: Validates test case verification results structure.

**Description**: This schema defines the complete verification result for a test case, including pass/fail status for each step, sequence, and the overall test case. Results use an enum-based format with distinct types for Pass, Fail, and NotExecuted states.

**Key Components**:
- **Test Case Level**: ID, description, overall pass/fail, step counts
- **Sequence Level**: Sequence ID, name, step results, all_steps_passed flag
- **Step Results** (enum variants):
  - `Pass`: Step passed with step number and description
  - `Fail`: Step failed with expected values, actual values, and reason
  - `NotExecuted`: Step was not executed

**Example**:

```json
{
  "test_case_id": "TC_001",
  "description": "Example test case",
  "sequences": [
    {
      "sequence_id": 1,
      "name": "Basic Test",
      "step_results": [
        {
          "Pass": {
            "step": 1,
            "description": "Execute command"
          }
        },
        {
          "Fail": {
            "step": 2,
            "description": "Check output",
            "expected": {
              "success": true,
              "result": "0",
              "output": "Success"
            },
            "actual_result": "1",
            "actual_output": "Error occurred",
            "reason": "Exit code mismatch: expected '0', got '1'"
          }
        },
        {
          "NotExecuted": {
            "step": 3,
            "description": "Cleanup"
          }
        }
      ],
      "all_steps_passed": false
    }
  ],
  "total_steps": 3,
  "passed_steps": 1,
  "failed_steps": 1,
  "not_executed_steps": 1,
  "overall_pass": false
}
```

**Usage Context**:
- Generated by: `TestVerifier::verify_test_case()`
- Used for: Detailed verification reporting and analysis
- Serialized to: JSON files for persistence and reporting

**Validation Tool**:
- `validate-json` binary: `validate-json verification_result.json schemas/verification-result.schema.json`

**Related Code**:
- `src/verification.rs` - `TestCaseVerificationResult`, `SequenceVerificationResult`, `StepVerificationResultEnum`
- `src/bin/test-verify.rs` - Test verification CLI tool

---

### 4. `verification-output.schema.json`

**Purpose**: Validates verification output files generated by the test executor.

**Description**: This schema is similar to `verification-result.schema.json` but specifically designed for the JSON output files produced by the test verification process. It provides a simpler step result format optimized for file output and external consumption.

**Key Components**:
- Test case identification and description
- Sequences with step results in simplified format
- Step result variants: `Pass`, `Fail`, `NotExecuted`
- Summary statistics: total, passed, failed, and not executed step counts
- Overall pass/fail status

**Example**:

```json
{
  "test_case_id": "TC_001",
  "description": "Example test case",
  "sequences": [
    {
      "sequence_id": 1,
      "name": "Basic Test",
      "step_results": [
        {
          "Pass": {
            "step": 1,
            "description": "Execute command"
          }
        },
        {
          "Fail": {
            "step": 2,
            "description": "Check output",
            "reason": "Exit code mismatch: expected '0', got '1'"
          }
        }
      ],
      "all_steps_passed": false
    }
  ],
  "total_steps": 2,
  "passed_steps": 1,
  "failed_steps": 1,
  "not_executed_steps": 0,
  "overall_pass": false
}
```

**Differences from verification-result.schema.json**:
- Simplified `Fail` structure (no `expected`, `actual_result`, `actual_output` fields)
- Optimized for file storage and external tool integration
- Stricter `additionalProperties: false` constraints

**Usage Context**:
- Generated by: Test verification tools during result export
- Output location: `testcases/test-runs/<test-case-id>/verification_output.json`
- Used by: CI/CD pipelines, reporting tools, external analysis systems

**Validation Tool**:
- `validate-json` binary: `validate-json verification_output.json schemas/verification-output.schema.json`

**Related Code**:
- `src/verification.rs` - Result serialization
- `src/bin/test-verify.rs` - Verification output generation
- `src/junit_xml_validator.rs` - JUnit XML export (alternative format)

---

## Common Usage Patterns

### Validating Test Cases in Bulk

```bash
# Validate all test case YAML files
validate-yaml testcases/*.yml --schema schemas/test-case.schema.json

# Watch mode for continuous validation during development
validate-yaml testcases/*.yml --schema schemas/test-case.schema.json --watch
```

### Validating Execution Logs

```bash
# Validate execution log
validate-json testcases/test-runs/TC_001/execution_log.json schemas/execution-log.schema.json
```

### Validating Verification Results

```bash
# Validate verification output
validate-json testcases/test-runs/TC_001/verification_output.json schemas/verification-output.schema.json
```

### Programmatic Validation (Rust)

```rust
use testcase_manager::SchemaValidator;

// Validate test case YAML
let validator = SchemaValidator::new()?;
validator.validate_chunk(yaml_content)?;

// Get detailed validation errors
let errors = validator.validate_with_details(yaml_content)?;
for error in errors {
    println!("Error at {}: {}", error.path, error.constraint);
}
```

## Integration with Build System

The schemas are validated as part of the build process:

```bash
# Run full validation suite
make lint

# Build project (includes validation)
make build

# Run tests
make test
```

## CI/CD Integration

Example GitLab CI configuration:

```yaml
validate:
  script:
    - validate-yaml testcases/*.yml --schema schemas/test-case.schema.json
    - validate-json testcases/test-runs/*/verification_output.json schemas/verification-output.schema.json
```

## Related Tools

### Command-Line Tools
- **`validate-yaml`**: YAML validation with watch mode support (Linux/macOS)
- **`validate-json`**: JSON validation against schemas
- **`test-verify`**: Test case verification and result generation
- **`test-orchestrator`**: Orchestrates test execution and verification

### Rust APIs
- **`SchemaValidator`**: Validates test case YAML structures
- **`TestCaseParser`**: Parses and validates test cases
- **`TestVerifier`**: Verifies execution results against expected outcomes
- **`TestExecutor`**: Executes test cases and generates logs

## Documentation References

- **Validation**: `docs/validation.md` - Comprehensive validation documentation
- **Quick Reference**: `docs/VALIDATE_YAML_QUICK_REF.md` - Quick reference for validate-yaml
- **Test Verification**: `docs/TEST_VERIFY_USAGE.md` - Test verification workflow
- **Watch Mode**: `docs/WATCH_MODE_COMPARISON.md` - Comparison of watch mode implementations

## Schema Compliance

All schemas follow JSON Schema specifications:
- **test-case.schema.json**: JSON Schema Draft-04
- **execution-log.schema.json**: JSON Schema Draft-07
- **verification-result.schema.json**: JSON Schema Draft-07
- **verification-output.schema.json**: JSON Schema Draft-07

## Contributing

When modifying schemas:
1. Ensure backward compatibility where possible
2. Update corresponding Rust structures in `src/models.rs` and `src/verification.rs`
3. Update related documentation
4. Run full test suite to validate changes: `cargo test --all-features`
5. Validate existing test cases against updated schemas
